### Abstract

With the recent prevalence and enhanced accessibility of Artificial Intelligence (AI) tools, the aim of this review is to assess both the benefits and risks of using AI tools in academic writing, thus allowing authors to make more informed decisions with regard to AI usage based on the current literature and the best available evidence.

Between February and April 2024, the authors conducted a narrative review of the academic literature using AI-focused keyword searches of databases including PubMed. Risks and benefits of using AI in academic writing were identified and subcategorized into four stages: planning, execution of research, drafting of a manuscript, and publication.

The literature suggests that AI tools, particularly large language models, provide several potential benefits at each stage of academic writing. This includes assistance in idea generation, data analysis, peer review, and drafting, with the potential to significantly improve overall efficiency. Significant challenges were also identified, including bias, plagiarism risk, and misleading AI-generated content (often referred to as hallucinations).

In conclusion, AI tools appear to present promising opportunities for improving academic writing and could potentially revolutionise the process in which academic research is conducted. Careful consideration of their limitations with legal and ethical implications is paramount—thus, the authors recommend that a collaborative effort led by the academic community is needed to establish best practice guidelines and regulatory frameworks for the responsible and effective implementation of AI tools in the process of scientific publications.

### Best Available Evidence

The emergence of artificial intelligence (AI) tools has marked a turning point in many domains, including academic writing. Their versatility ranges from narrow in scope and purpose, such as software used to guide self-driving cars, to broadly applicable tasks, such as large language models (LLMs) like OpenAI’s ChatGPT.

LLMs can produce natural-sounding language output in response to wide-ranging user prompts—they achieve this by using statistical patterns of natural language learned from vast amounts of training data to predict the appropriate response to a given prompt.<sup style="color:#44A6B2;">1 </sup> These tools could strongly influence the field of academic writing—bringing many unique benefits and challenges to researchers hoping to use their capabilities to improve their writing.

This review is focused primarily on academic papers discussing the application of AI tools in academic writing. The authors decided to allow the inclusion of various types of sources, including editorials, commentaries, and discussion articles, given that the capabilities of these tools and their relative novelty make the topic quite inviting, yet ethically complex. The goal is to capture a range of perspectives, recognising that the topic invites theoretical and speculative contributions from experts. Additionally, preliminary scoping showed that limiting sources to trials and quantitative studies would provide insufficient material for discussion and analysis. One limitation of this methodology is the absence of a standardized search process, which means the results may not support a repeatable analysis but instead serve as a broad summary for the reader.

Papers were retrieved by conducting simple keyword searches (such as “artificial intelligence AND academic writing”) between February and April 2024. Subsequently, the authors retrieved relevant papers that had been cited by the papers identified in initial searches. Finally, using the “cited by” function, more recent papers that had been missed in the initial searches were identified. These papers were screened for relevance, and the remaining papers were processed to extract the key opportunities and risks that they discussed regarding AI use in academic writing. These data were organised into four stages of academic writing to serve as an organisational framework to guide the discussion of how AI may contribute to different aspects of writing.

#### Stages of Academic Writing

Writing an academic piece will be subdivided into four main stages: planning, execution of research, drafting of a manuscript, and publication. This framework was designed by the authors to serve as an organisational framework to guide the discussion of how AI may contribute to different aspects of writing. AI can assist with each of these stages whilst bringing unique challenges to them (Tables 1 and 2).  

The planning stage encompasses the preparation of a piece of academic writing- everything that happens before any words are written. This includes deciding what to research and write about, the process of familiarising oneself with a new topic before a formal literature search and designing protocols for systematic reviews and experimental procedures.  
The execution of research is the stage that the researcher performs to gain the information that they will write about when drafting the piece. This involves a formal literature review and data handling.  

The drafting stage involves the actual writing of the piece. This entails outlining a piece of writing, producing a first draft of each section of the piece, and fine-tuning the readability of the finished product. Data presentation will be considered part of this stage of the process.  

The publication stage is the subsequent process of getting the paper from the final draft to published work. This section will discuss the effect of AI on peer review, along with the ethical and legal considerations of AI tool authorship.  

#### Planning

The first step of academic writing is planning—this includes deciding a topic about which to write, developing a background understanding of the topic to be discussed, and study design. AI tools can be used for many aspects of this process.

##### Opportunities

It has been demonstrated that certain AI tools, such as the LLM ChatGPT, can aid in topic selection, providing novel suggestions for areas of research within a given field and offering valuable support to researchers in the early stages of planning.<sup style="color:#44A6B2;">2 </sup> This can take many forms, such as having the LLM answer questions about the current areas of interest in each area of study and providing keywords for subareas that researchers can consider writing about. This allows a human user, in developing a research question, to use an AI tool to elaborate on an idea and brainstorm. It is important to note that all outputs from LLMs are a product of their training data, creating a philosophical point of contention about the limited ability of these models to truly innovate. However, with up-to-date training data, AI tools can effectively highlight unexplored research areas and provide a human researcher with a useful starting point in developing a research question.

AI can help overcome another barrier to initiating the writing process: gaining a sufficient background understanding of the specific area to be studied. It has been noted that ChatGPT is a powerful self-learning tool.<sup style="color:#44A6B2;">3 </sup> This is partly because of its ability to be used as an improved search engine, in which questions can be posed conversationally and receive specific answers.<sup style="color:#44A6B2;">4 </sup> LLMs, such as GPT-4, are also able to take PDF files of long and complex texts and provide summaries, allowing quick extraction of the major points. This could be a great time-saving tool for researchers trying to decide which papers are relevant for their self-study while trying to understand a new topic.

Once the research question has been formulated, AI can assist in study design. ChatGPT can provide suggestions for research protocols that are rigorous and feasible alongside sensible suggestions for data collection and analysis.<sup style="color:#44A6B2;">5 </sup> ChatGPT also seems to be able to weigh up the strengths of a study design it has been provided, giving valuable points of reflection for researchers.<sup style="color:#44A6B2;">5 </sup> In an article in which ChatGPT is used to assist in drafting an article for Reproductive Biomedicine Online, the LLM was able to return suggestions of possible analyses that could be performed with the data, once the dataset was described, while also suggesting other factors that could be investigated and included as covariates in the statistical analysis.<sup style="color:#44A6B2;">6 </sup>

##### Risks

When used for topic selection, AI may struggle to understand the nuanced context of a field of research, potentially leading to misguided suggestions. Furthermore, heavy dependence on AI for idea generation may result in a lack of creativity and original thought in the research process.

Moreover, these AI tools tend to invent information convincingly, confidently providing false information mixed in with correct information, a potentially dangerous risk when using these tools for self-education. This phenomenon is referred to as “hallucination,” and, in the case of LLMs, is essentially because the AI is not providing information from a database of objective facts, but rather predicting what “sounds best” in response to a given question. In one study it was observed that when GPT-3 is asked for references in the context of generating research protocols, only 61% of these references had valid DOIs, with only 16% turning up on internet search, valid DOI or not.<sup style="color:#44A6B2;">7 </sup> In a writing experiment on anatomical variations, ChatGPT produced plausible arguments but admitted to fabricating data to enhance the impact of the research, underlining the fact that these models tend to speculate in cases of uncertainty.<sup style="color:#44A6B2;">8 </sup> Observations of this kind have prompted experts to highlight the importance of fact-checking AI-generated output, for which accuracy is far from a foregone conclusion.<sup style="color:#44A6B2;">1 </sup>

#### Execution of Research

When conducting research, AI can assist in summarising the current literature and the handling of data in the execution phase of academic research. There are issues associated with AI-generated content including hallucination and bias which will be explored below.

##### Opportunities

AI can assist academic researchers in summarising and understanding the literature, accelerating the writing of the literature review portion of a paper and thus saving researchers a significant portion of time.

In addition to guiding literature reviews and research, AI tools can be used to assist with data handling. One major benefit of using AI in this regard is processing speed—AI can process vast amounts of data more quickly, spotting patterns that human authors may not. AI tools such as Copilot can also help with code writing for data handling.<sup style="color:#44A6B2;">9 </sup> LLMs such as ChatGPT can also produce code, though often with mistakes. ChatGPT can, however, self-correct once errors are pointed out.<sup style="color:#44A6B2;">10 </sup>

##### Risks

Current LLMs are known to hallucinate references when asked to summarise literature.<sup style="color:#44A6B2;">6, 7 </sup> Multi-purpose, general-use AI such as LLMs may simply lack the specific functionalities to allow them to be useful in the specific task of literature review. To address this, AI tools that are narrower in scope could be used. A PubMed GPT, an LLM trained on data from the PubMed database, is currently in development; however, due to the greater number of parameters needed for this kind of model, trade-offs are necessary between practicality and model sophistication.<sup style="color:#44A6B2;">11 </sup> Other specialised AI tools do exist, such as the AI used in Semantic Scholar to produce short summaries of academic papers with good accuracy.<sup style="color:#44A6B2;">9 </sup>

Another concern in using AI to assist as a research assistant is bias—the output of an LLM will reflect biases in its training data, which could bias future research direction. This kind of bias is demonstrable.<sup style="color:#44A6B2;">12 </sup> It has been shown that popular LLMs (e.g., GPT-2 and CTRL-OPN) show greater bias towards various social groups than human-generated sample text from Wikipedia.<sup style="color:#44A6B2;">13 </sup> Various efforts are being taken to produce anti-bias measures for AI tools, such that they can be more widely used with less concern for propagating bias—these include machine-centric solutions and human feedback-based fine-tuning models.<sup style="color:#44A6B2;">12,14,15 </sup> Improving AI explainability could be a key measure in this regard, as it would improve the ability of researchers to understand why a given AI model responds with a given bias.<sup style="color:#44A6B2;">16 </sup> The lack of explainability in how AI may choose to handle data produces ethical and practical questions, possibly allowing biased data processing that can go unaddressed by researchers if used irresponsibly.<sup style="color:#44A6B2;">17 </sup>

#### Drafting of a Manuscript

AI tools can help with an initial starting point for the manuscript, providing an initial draft to help overcome any difficulties with putting pen to paper. Naturally, this will speed up the writing process, as it can quickly draft titles, abstracts, conclusions, and sections of papers. However, this has raised concerns about plagiarism, as well as the potential impact of AI on the diversity of writing styles and data communication, potentially undesirably homogenising the literature.<sup style="color:#44A6B2;">18 </sup>

##### Opportunities

The ability of LLMs to generate fluent sentences in natural language could be a huge boon to the scientific writing community, massively improving accessibility for both authors and readers.<sup style="color:#44A6B2;">19 </sup> One clear benefit to this is for researchers who are not native English speakers, for whom producing grammatically correct sentences is an arduous task. Non-native English speakers already often need to use a toolkit of services that natives do not to write correctly.<sup style="color:#44A6B2;">18 </sup>

As authors would still be ultimately responsible for the output that gets published, this wouldn’t compromise the integrity of those authors insofar as rendering them less accountable for what they write. An AI language tool called Grammarly, which acts as an advanced spell-checker, providing suggestions for ways to more naturally phrase sentences, was used in a randomised controlled trial which demonstrated that it was effective in improving engagement, efficacy and emotional responses of second language students towards writing—similar results would be expected from use of LLMs as proof-readers or first drafters.<sup style="color:#44A6B2;">20 </sup> Using AI tools for such tasks is ideal, as it overcomes basic human error, which is most prominent in repetitive tasks like proofreading due to failure to sustain attention.<sup style="color:#44A6B2;">21 </sup>

LLMs such as ChatGPT can write technical texts while sounding credible. When ChatGPT was used to generate fake scientific abstracts based on titles from high-impact medical journals, a plagiarism detector found the content to be 100% original and sceptical human reviewers only identified 68% of the abstracts as being fake.<sup style="color:#44A6B2;">22 </sup> This demonstrates the ability of ChatGPT to sound credible when writing technically. It has been further suggested that in the future, AI will be helpful in data communication, e.g., via the production of AI-generated visual aids.<sup style="color:#44A6B2;">23 </sup>

##### Risks

A potential risk of the use of AI for communicative purposes, both in data communication and language processing, is that it could reduce the diversity of styles that currently enrich the literature. If a large proportion of authors use AI to produce the body of their text, for example, then this will cause a homogenisation of the literature—eventually, everything could sound the same.

There are further potential drawbacks surrounding the use of AI tools in the drafting of a manuscript for a scientific article. Following recent examples in the media of AI journalism producing factual errors and committing extensive plagiarism, concerns about the use of AI in writing are heightened.<sup style="color:#44A6B2;">24 </sup> There are concerns both that the use of AI-generated content is inherently plagiarism (as the author is plagiarising the AI) and/or that AI is likely to plagiarise existing works in its output.<sup style="color:#44A6B2;">25 </sup> One could argue that the latter comment is an unfair standard. Though an LLM is trained upon the works of others, it still generates unique responses itself—as humans, we also generate language based upon a model, albeit more complex, that we have produced from vast amounts of input—a key difference being that we inform our speech with knowledge and convictions that are held separately, rather than just statistically mimicking the patterns of our native language. This opens the floor to a more philosophical debate on the nature of originality itself. One could conversely argue that, although AI-generated text may appear 100% original to a plagiarism detector, that nevertheless the AI is merely rephrasing its training data, without adding anything new—this is still a violation of the integrity of an author, even if not detectable as plagiarism.<sup style="color:#44A6B2;">22, 26 </sup> It has been argued that the non-human nature of AI tools causes them to be “precluded from actuality” in such a way that they are therefore unable to produce any original conclusions based upon innate knowledge or reasoning.<sup style="color:#44A6B2;">23 </sup> If this is true, AI tools can, therefore, certainly not be expected to reason and conclude in any way that will meaningfully contribute to society.

Another significant concern in using AI to draft a manuscript is that of hallucination, which is discussed more extensively above in Section 3. This is arguably less of an issue in cases in which the use of AI is restricted to language assistance—for instance, if a researcher provides an LLM with a list of detailed points to be expressed in natural language, which the researcher then curates. However, this could still produce significant miscommunications when LLMs are provided with more open prompts, which are more likely to lead them to fabricate, such as “I’m writing a paper on the benefits and drawbacks of AI in academic writing. Please draft the section regarding the pros and cons of using AI in the drafting of the initial manuscript.”

#### Publication and Authorship

Once the manuscript is complete, the process of reviewing and publishing the paper begins. Apart from tools exploited in submission, designing, and advertising of scientific publications, AI’s role in this part of academic writing is an area of controversy. It has been suggested that it is inevitable that AI tools will soon be capable of producing works that can pass peer review.<sup style="color:#44A6B2;">4 </sup> In this section, we will discuss the potential role of AI tools in the peer-review process and the debates on the status of LLMs as authors, touching on both legal and ethical considerations.

##### Opportunities

Aside from whether AI can produce outputs which pass peer review, there is the question of how AI could help peer reviewers. It may never be able to completely replace peer reviewers, as its lack of personhood, by definition, prevents it from being considered a peer. AI tools could still accelerate the process of peer review and thus improve the pace at which research can be advanced to publication. As proof of concept, a neural network model was trained on over 3,000 papers, along with the reviewers’ evaluations for these papers, then this network was tested on its ability to predict the peer-review outcome, which it often succeeded at.<sup style="color:#44A6B2;">27 </sup> Such examples demonstrate the potential of this technology. Even if these concerns prevent AI from being used directly in the peer review process, these tools can still be used by authors to help them prepare their writing for peer review—in this sense, AI tools could take the role of a critical colleague, unofficially criticising a piece of writing to help authors make the best version of their work possible, before entering a formal peer review process. Future research should be directed towards understanding the capacity of AI in peer review.

##### Risks

In these instances, however, specific care should be taken to account for biases—for instance, if AI tools are to be trained on peer-review evaluations, the output of this AI may reflect and perpetuate underlying biases in academic review, such as status quo bias.<sup style="color:#44A6B2;">28 </sup> 

Whether or not an AI tool can be considered the author of an academic work is a matter of debate. Some papers have already been published listing ChatGPT as a co-author, raising legal, philosophical, and regulatory questions about the nature of authorship and the intellectual property rights of AI-generated content.<sup style="color:#44A6B2;">29 </sup> Citing ChatGPT as an author in this way sets a precedent of acknowledging the contributions of AI to the work, which is a change geared towards academic transparency. However, there are other ways one could acknowledge the use of AI tools, such as in the methods section of a paper. These approaches highlight an important distinction—are we to consider AI models as tools or collaborators? Many journals and organisations are developing policies that err on the side of refusing to list nonhuman technologies as authors, with *Nature* declaring the listing of ChatGPT as an author in one of their papers as a mistake, which would be corrected.<sup style="color:#44A6B2;">25, 30, 31 </sup> The reasons cited for these decisions are variable, but one common theme is that authorship, especially in a scientific paper, implies a taking of responsibility for the published material, which AI cannot assume.

#### Broader Questions and Implications

The use of AI in scientific writing has stirred up much discussion concerning its implications for the future of the industry. Some issues identified are those seen whenever a new process can be automated, such as fears of job loss.<sup style="color:#44A6B2;">32 </sup> However, some concerns are also specific to AI technology—especially concerns regarding the tendency of AI technologies to be less transparent, i.e., to have lower levels of explainability.<sup style="color:#44A6B2;">17 </sup> These matters all raise the question of what preparations we, as a community, should be making for the future considering this new wave of technology; we should consider matters of regulation and proper usage of AI technologies in academia.

A vibrant discussion has been sparked due to relatively widespread concern about the potential for job loss from increased usage of AI tools.<sup style="color:#44A6B2;">32 </sup> In a survey, 25% of medical students acknowledge that AI’s effect on the job market is actively influencing their choice of speciality.<sup style="color:#44A6B2;">33 </sup> Furthermore, if AI tools are to be used in research and academic writing, there is a concern that this will result in a decline of critical thinking and writing skills among researchers due to overreliance on these tools.<sup style="color:#44A6B2;">34 </sup> So, what should be done? Perhaps ‘skill loss’ is not the right lens through which to consider these issues. Increased usage of AI tools could result in a ‘rebalancing’ of the skillset used by academic researchers, as some areas (e.g., analysis and evaluation) become more important, and other areas (e.g., drafting and literature searching) become less important—in this case, the onus is placed upon the wider academic community to consider which skills can and should be considered essential for researchers in the new technological climate.<sup style="color:#44A6B2;">35 </sup> For example, it has been suggested that the ability of ChatGPT to pass the USMLE demonstrates that the exam focuses purely on factual types of knowledge and that we should expect greater levels of analytical reasoning in medical education.<sup style="color:#44A6B2;">36 </sup> If the use of such tools does change the job market in academia and the types of skills that produce success, perhaps the community simply needs to reassess the duties and priorities of an ideal researcher. Such discussion does produce concerns of a split in the research community between the AI-competent and those not AI-competent.<sup style="color:#44A6B2;">23 </sup> A greater concern should be afforded to the division in the community between those with access to the best AI tools and those who do not, should high-quality AI tools be restricted with paywalls or geographical restrictions.

One other salient issue regarding the use of AI stems from the fact that most AI systems produce fundamentally unexplainable outcomes due to their ‘black box’ nature. Ethically, this could present an issue as research methods inherently need to be transparent to allow for proper accountability and the opportunity for criticism.<sup style="color:#44A6B2;">17 </sup> For instance, if an AI algorithm is used in data analysis for a research paper, the AI may identify patterns and correlations that are not immediately identifiable or explainable by current scientific understanding—this provides an opportunity to progress our current knowledge but also could lead to a reliance on conclusions that cannot be transparently traced to underlying data. This produces an ethical dilemma—if such conclusions are used to inform medical decisions, for instance, is the ability of a patient to give informed consent inherently compromised because they are deciding based on unexplainable evidence? As such, efforts should be made to improve the explainability of any AI models used in research: this can be achieved through a variety of measures, such as creating interpretable models and finding ways to visualise model outputs to better understand their function.<sup style="color:#44A6B2;">16 </sup> 

There are several ways we should prepare for the future regarding the use of AI in academic writing. Appropriate regulation should be implemented sooner than later, as it has been stipulated that AI will soon be capable of writing papers that can pass peer review—thus, it would be better to know where we stand on the regulatory front before this point is reached.<sup style="color:#44A6B2;">4 </sup> Oversaturation with AI-generated content is an increasing concern, which will worsen as AI-generated content increases in sophistication—thus, appropriate regulations will be necessary to limit this and prevent AI-powered paper mills from developing.<sup style="color:#44A6B2;">37 </sup> What form this regulation may take is an open question—discussion is centered around what uses of AI should be controlled and utilised and which uses we should gear education of the community towards.<sup style="color:#44A6B2;">38 </sup> The current trend seems to be towards that of transparency in the use of AI, encouraging researchers to acknowledge if, when, and how they have used AI tools to assist in the composition of their research. This could take specific forms, such as watermarking a paper with a symbol to highlight if AI has been used in its creation.<sup style="color:#44A6B2;">23 </sup> Encouraging proper usage of AI is therefore clearly important to meet ethical and regulatory standards, with naïve use leading to potential problems like unintentional copyright infringement.<sup style="color:#44A6B2;">3,34 </sup> Efficient and effective use of AI should also be encouraged. Researchers should be trained on how to get the most out of using AI tools. For example, in the use of LLMs, ‘prompt engineering’ is incredibly important—this term is used to describe how careful crafting of the prompt given to an LLM will shape its response to be more or less useful and appropriate to the given context.<sup style="color:#44A6B2;">39 </sup> When using ChatGPT, for instance, it’s important to ask it to ‘tailor’ its response to the context of the inquiry.<sup style="color:#44A6B2;">37 </sup> 

### Conclusion

In conclusion, AI tools have utility at every stage of academic writing. It appears that their use in academic writing could improve efficiency and save time but there are several ethical and practical considerations, specifically regarding AI authorship as well as plagiarism and reliability of output. As such, there is a great need for critical oversight and fact-checking. The benefits of AI usage are strong, and with mindful adoption, efforts can be made to mitigate its risks and maximise its benefits.

#### Take-Home Messages

- Large language models (LLMs) can be used to further develop a research question.  
- AI tools can provide useful suggestions for data analysis and study design.  
- LLMs often convincingly generate false information, so fact-checking is needed.  
- LLMs can be used to develop a search strategy but are currently unreliable at performing literature reviews themselves.  
- LLMs can massively improve drafting efficiency and accessibility, especially for authors writing in their second language.  
- AI tools can effectively assist in manuscript preparation for peer review, but careful implementation is needed to ensure that this doesn’t lead to bias propagation.

Table 1: Opportunities offered by AI use in academic writing with suggested reading.
| Stage of Academic Writing      | Opportunities                                                                                 | Citation               |
|--------------------------------|-----------------------------------------------------------------------------------------------|------------------------|
| **Planning**                   | Topic selection<br>- Developing background understanding<br>- Study design                  | (2)<br> (3, 4)<br> (5, 6)            |
| **Execution of research**      | Literature summary<br>- Data handling/code writing support                                  | (9)<br> (5, 6, 9, 16) |
| **Drafting of a manuscript**   | Rapid drafting of high-quality text<br>- Improved language accessibility                     | (22)<br> (18–20)     |
| **Publication and authorship** | AI-assisted peer review                                                                     | (27)                 |

Table 2: Risks posed by AI use in academic writing with suggested reading.
| **Risk of AI Use in Academic Writing**                                                                            | **Citation**         |
|-------------------------------------------------------------------------------------------------------------------|----------------------|
| Hallucination                                                                                                     | (1, 6, 7, 8)       |
| Bias                                                                                                              | (12–15)            |
| Lack of explainability                                                                                            | (16, 17)           |
| Plagiarism                                                                                                        | (18, 24–26)        |
| Oversaturation with AI-generated content                                                                          | (37)               |
| Change of levels of key skills in academics                                                                       | (34– 36)           |


###### Disclosure Statement  
The authors declare no conflicts of interest.

### References

1. Birhane A, Kasirzadeh A, Leslie D, Wachter S. Science in the age of large language models. *Nat Rev Phys.* 2023 May;5(5):277–80. doi:10.1038/s42254-023-00581-4.  
2. Huang J, Tan M. The role of ChatGPT in scientific communication: writing better scientific review articles. *Am J Cancer Res.* 2023 Apr 15;13(4):1148–54.  
3. Fijačko N, Gosak L, Štiglic G, Picard CT, Douma MJ. Can ChatGPT pass the life support exams without entering the American Heart Association course? *Resuscitation.* 2023;185:109732. doi:10.1016/j.resuscitation.2023.109732.  
4. Gordijn B, Have H ten. ChatGPT: evolution or revolution? *Med Health Care Philos.* 2023;26(1):1–2. doi:10.1007/s11019-023-10136-0.  
5. Rice S, Winter SR, Rice C. The Advantages and Limitations of Using ChatGPT to Enhance Technological Research. SSRN; 2023.  
6. Altmäe S, Sola-Leyva A, Salumets A. Artificial intelligence in scientific writing: a friend or a foe? *Reprod Biomed Online.* 2023.  
7. Athaluri SA et al. Exploring the Boundaries of Reality: Investigating the Phenomenon of Artificial Intelligence Hallucination in Scientific Writing Through ChatGPT References. *Cureus.* 2023;15(4):e37432. doi:10.7759/cureus.37432.  
8. Shoja MM, Van De Ridder JMM, Rajput V. The Emerging Role of Generative Artificial Intelligence in Medical Education, Research, and Practice. *Cureus.* 2023.  
9. Hutson M. Could AI help you to write your next paper? *Nature.* 2022;611(7934):192–3. doi:10.1038/d41586-022-03479-w.  
10. Macdonald C, Adeloye D, Sheikh A, Rudan I. Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis. *J Glob Health.* 2023.  
11. Gupta K. Stanford and MosaicML Researchers Announce the Release of PubMed GPT, a Purpose-Built AI Model Trained to Interpret Biomedical Language. *MarkTechPost.* 2022.  
12. Liang PP, Wu C, Morency LP, Salakhutdinov R. Towards Understanding and Mitigating Social Biases in Language Models. *arXiv;* 2021.  
13. Dhamala J, Sun T, Kumar V, et al. BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation. In: *FAccT ’21.* 2021.  
14. Belenguer L. AI bias: exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry. *AI Ethics.* 2022;2(4):771–87. doi:10.1007/s43681-022-00138-8.  
15. Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback. *arXiv;* 2022.  
16. Holzinger A, Keiblinger K, Holub P, et al. AI for life: Trends in artificial intelligence for biotechnology. *New Biotechnol.* 2023;74:16–24. doi:10.1016/j.nbt.2023.02.001.  
17. Amann J, Blasimme A, Vayena E, et al. Explainability for artificial intelligence in healthcare: a multidisciplinary perspective. *BMC Med Inform Decis Mak.* 2020;20(1):310. doi:10.1186/s12911-020-01332-6.  
18. Sinclair BJ. Letting ChatGPT do your science is fraudulent (and a bad idea), but AI-generated text can enhance inclusiveness in publishing. *Curr Res Insect Sci.* 2023;3:100057. doi:10.1016/j.cris.2023.100057.  
19. Chen TJ. ChatGPT and other artificial intelligence applications speed up scientific writing. *J Chin Med Assoc.* 2023;86(4):351. doi:10.1097/JCMA.0000000000000900.  
20. Nazari N, Shabbir MS, Setiawan R. Application of Artificial Intelligence–powered digital writing assistant in higher education: randomized controlled trial. *Heliyon.* 2021;7(5):e07014. doi:10.1016/j.heliyon.2021.e07014.  
21. Langner R, Eickhoff SB. Sustaining attention to simple tasks: A meta-analytic review of the neural mechanisms of vigilant attention. *Psychol Bull.* 2013;139(4):870–900. doi:10.1037/a0030694.  
22. Gao CA, Howard FM, Markov NS, et al. Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers. *bioRxiv.* 2022;2022.12.23.521610. doi:10.1038/s41746-023-00819-6.  
23. Wohlfarth B, Streit SR, Guttormsen S. Artificial Intelligence in Scientific Writing: A Deuteragonistic Role? *Cureus.* 2023;15(9):e45513. doi:10.7759/cureus.45513.  
24. Christian J. Futurism. CNET’s AI Journalist Appears to Have Committed Extensive Plagiarism. 2023. Available from: https://futurism.com/cnet-ai-plagiarism.  
25. H Holden Thorp. ChatGPT is fun, but not an author. *Science.* 2023;379(6630):313. doi:10.1126/science.adg7879.  
26. Salvagno M, Taccone FS, Gerli AG. Can artificial intelligence help for scientific writing? *Crit Care.* 2023;27(1):75. doi:10.1186/s13054-023-04380-2.  
27. Checco A, Bracciale L, Loreti P, et al. AI-assisted peer review. *Hum Soc Sci Commun.* 2021;8(1):1–11. doi:10.1057/s41599-020-00703-8.  
28. Godefroid ME, Plattfaut R, Niehaves B. How to measure the status quo bias? A review of current literature. *Manag Rev Q.* 2023;73(4):1667–711. doi:10.1007/s11301-022-00283-8.  
29. Zhavoronkov A. Rapamycin in the context of Pascal’s Wager: generative pre-trained transformer perspective. *Oncoscience.* 2022;9:82–4. doi:10.18632/oncoscience.571.  
30. Flanagin A, Bibbins-Domingo K, Berkwits M, Christiansen SL. Nonhuman “Authors” and Implications for the Integrity of Scientific Publication and Medical Knowledge. *JAMA.* 2023;329(8):637. doi:10.1001/jama.2023.1344.  
31. Stokel-Walker C. ChatGPT listed as author on research papers: many scientists disapprove. *Nature.* 2023;613(7945):620–1. doi:10.1038/d41586-023-00107-z.</span>
32. Deranty JP, Corbin T. Artificial intelligence and work: a critical review of recent research from the social sciences. *AI Soc.* 2022;37(3):653–665. doi:10.1007/s00146-022-01496-x.  
33. Mehta N, Harish V, Bilimoria K, et al. Knowledge of and attitudes on artificial intelligence in healthcare: a provincial survey study of medical students. *medRxiv.* 2021;2021.01.14.21249830. doi:10.15694/mep.2021.000075.1.  
34. Dergaa I, Chamari K, Zmijewski P, Saad HB. From human writing to artificial intelligence generated text: examining the prospects and potential threats of ChatGPT in academic writing. *Biol Sport.* 2023;40(2):615–22. doi:10.5114/biolsport.2023.125623.  
35. van Dis EAM, Bollen J, Zuidema W, van Rooij R, Bockting CL. ChatGPT: five priorities for research. *Nature.* 2023;614(7947):224–6. doi:10.1038/d41586-023-00288-7.  
36. Mbakwe AB, Lourentzou I, Celi LA, Mechanic OJ, Dagan A. ChatGPT passing USMLE shines a spotlight on the flaws of medical education. *PLOS Digit Health.* 2023;2(2):e0000205. doi:10.1371/journal.pdig.0000205.  
37. Arachchige ASPM, Stomeo N. Exploring the Opportunities and Challenges of ChatGPT in Academic Writing: Reply to Bom et al. *Nucl Med Mol Imaging.* 2023;57(5):213–4. doi:10.1007/s13139-023-00816-3.  
38. Liu H, Azam M, Bin Naeem S, Faiola A. An overview of the capabilities of ChatGPT for medical writing and its implications for academic integrity. *Health Inf Libr J.* 2023;40(4):440–6. doi:10.1111/hir.12509.  
39. Heston TF, Khun C. Prompt Engineering in Medical Education. *Int Med Educ.* 2023;2(3):198–205. doi:10.3390/ime2030019.  
